{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d834ea-968d-4f70-b9a4-b3bad36c6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的包\n",
    "import dxpy\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# ======================================================================\n",
    "# 第一步：在这里定义您需要提取的所有表型ID\n",
    "# 这也是您唯一需要修改的地方！\n",
    "# ======================================================================\n",
    "target_field_ids = ['31', '34', '52'] + [str(i) for i in range(22000, 22020)] # 示例ID，请替换为您自己的列表\n",
    "print(f\"目标：提取 {len(target_field_ids)} 个表型ID的数据。\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 第二步：自动获取数据集信息和数据字典 (与蛋白质脚本相同)\n",
    "# ======================================================================\n",
    "print(\"\\n--- 正在自动查找数据集信息... ---\")\n",
    "# 自动发现数据集ID和项目ID\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")\n",
    "project_id = dxpy.find_one_project()[\"id\"]\n",
    "dataset = f\"{project_id}:{dispensed_dataset['id']}\"\n",
    "\n",
    "# 使用-ddd命令仅下载数据字典\n",
    "print(\"--- 正在下载数据字典... ---\")\n",
    "cmd = [\"dx\", \"extract_dataset\", dataset, \"-ddd\", \"--delimiter\", \",\"]\n",
    "subprocess.check_call(cmd)\n",
    "\n",
    "# 读取数据字典\n",
    "path = os.getcwd()\n",
    "data_dict_csv = glob.glob(os.path.join(path, \"*.data_dictionary.csv\"))[0]\n",
    "data_dict_df = pd.read_csv(data_dict_csv)\n",
    "print(\"数据字典下载并读取完毕。\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 第三步：根据您定义的ID列表，从字典中筛选出完整的字段名\n",
    "# ======================================================================\n",
    "print(\"\\n--- 正在从字典中筛选您指定的表型字段... ---\")\n",
    "# 注意：UKB数据字典中的Field ID是整数类型，先将我们的目标ID转换为整数\n",
    "target_field_ids_int = [int(i) for i in target_field_ids]\n",
    "\n",
    "# 从字典中筛选出field_id在我们目标列表中的行\n",
    "pheno_fields_df = data_dict_df[data_dict_df[\"field_id\"].isin(target_field_ids_int)]\n",
    "\n",
    "# 构造用于查询的字段名 (格式为 \"entity.name\")\n",
    "# 同时加入'eid'字段作为参与者ID\n",
    "field_names_str = ['participant.eid'] + [\n",
    "    f\"{row['entity']}.{row['name']}\" for index, row in pheno_fields_df.iterrows()\n",
    "]\n",
    "field_names_query = \",\".join(field_names_str)\n",
    "print(f\"成功匹配到 {len(field_names_str) - 1} 个字段名，准备提取。\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 第四步：生成SQL并使用Spark高效提取数据 (与蛋白质脚本相同)\n",
    "# ======================================================================\n",
    "print(\"\\n--- 正在生成SQL查询并使用Spark提取数据... ---\")\n",
    "# 初始化Spark\n",
    "conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# 使用 --sql 参数生成查询语句\n",
    "cmd = [\n",
    "    \"dx\", \"extract_dataset\", dataset,\n",
    "    \"--fields\", field_names_query,\n",
    "    \"--delimiter\", \",\",\n",
    "    \"--output\", \"extracted_phenotypes.sql\", # 输出的SQL文件名\n",
    "    \"--sql\",\n",
    "]\n",
    "subprocess.check_call(cmd)\n",
    "\n",
    "# 读取SQL文件并用Spark执行\n",
    "with open(\"extracted_phenotypes.sql\", \"r\") as file:\n",
    "    retrieve_sql = \"\".join([line.strip() for line in file])\n",
    "\n",
    "temp_df = spark.sql(retrieve_sql.strip(\";\"))\n",
    "\n",
    "# 转换为Pandas DataFrame\n",
    "pheno_pdf = temp_df.toPandas()\n",
    "print(\"--- 数据提取完毕！---\")\n",
    "print(f\"成功提取到 {pheno_pdf.shape[0]} 行, {pheno_pdf.shape[1]} 列的数据。\")\n",
    "print(\"数据预览:\")\n",
    "print(pheno_pdf.head())\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 第五步：保存并上传结果\n",
    "# ======================================================================\n",
    "# 定义输出文件名和上传路径\n",
    "output_filename = 'my_phenotypes.csv'\n",
    "output_upload_path = '/my_results/' # 您可以修改为您想要的文件夹\n",
    "\n",
    "# 写出文件\n",
    "print(f\"\\n--- 正在将数据保存为 {output_filename}... ---\")\n",
    "pheno_pdf.to_csv(output_filename, sep='\\t', na_rep='NA', index=False, quoting=3)\n",
    "\n",
    "# 上传到RAP项目文件夹中\n",
    "print(f\"--- 正在上传文件到 {output_upload_path}... ---\")\n",
    "# 使用subprocess调用dx upload，比%%bash更通用\n",
    "upload_cmd = [\"dx\", \"upload\", output_filename, \"-p\", \"--path\", output_upload_path, \"--brief\"]\n",
    "subprocess.check_call(upload_cmd)\n",
    "print(\"--- 脚本执行完毕！---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
